{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir Datasets\n",
    "!mv test.csv Datasets/test_set.csv\n",
    "!mv train.csv Datasets/train_set.csv\n",
    "!mv val.csv Datasets/validation_set.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smPB1gwQtXCQ"
   },
   "source": [
    "# Fine-tune CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71838,
     "status": "ok",
     "timestamp": 1714471651217,
     "user": {
      "displayName": "Roumeliotis Konstadinos",
      "userId": "17264923090131634662"
     },
     "user_tz": -180
    },
    "id": "Vjx60ntBha1W",
    "outputId": "61dcae4a-d25f-45d8-c145-4b57924ceab7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10627509576847154832\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15510929408\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14326477267733473123\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\"\n",
      "xla_global_id: 416903419\n",
      "]\n",
      "Mounted at /content/gdrive\n",
      "Epoch 1/3\n",
      "595/595 [==============================] - 18s 23ms/step - loss: 0.5288 - accuracy: 0.8656 - val_loss: 0.4035 - val_accuracy: 0.8653\n",
      "Epoch 2/3\n",
      "595/595 [==============================] - 6s 10ms/step - loss: 0.3877 - accuracy: 0.8659 - val_loss: 0.3854 - val_accuracy: 0.8653\n",
      "Epoch 3/3\n",
      "595/595 [==============================] - 4s 7ms/step - loss: 0.3786 - accuracy: 0.8659 - val_loss: 0.3789 - val_accuracy: 0.8653\n",
      "Training Loss: [0.5288362503051758, 0.38770896196365356, 0.3785526752471924]\n",
      "Validation Loss: [0.4034520387649536, 0.3853952884674072, 0.37890899181365967]\n",
      "Validation Accuracy: [0.8653198480606079, 0.8653198480606079, 0.8653198480606079]\n",
      "Training time: 65.40 seconds\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense \n",
    "from tensorflow.python.client import device_lib \n",
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    " # For replication purposes\n",
    "tf.random.set_seed(420)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "random.seed(0)\n",
    "\n",
    "class CNNTraining:\n",
    "    def __init__(self, learning_rate, epochs, batch_size, max_len, feature_col, label_col):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len\n",
    "        self.history = None\n",
    "\n",
    "    def load_data(self, train_file_path, val_file_path):\n",
    "        train_df = pd.read_csv(train_file_path)\n",
    "        val_df = pd.read_csv(val_file_path)\n",
    "\n",
    "        # Extracting text and labels from training data\n",
    "        self.train_texts = train_df[feature_col].tolist()\n",
    "        self.train_labels = train_df[label_col].values\n",
    "\n",
    "        # Extracting text and labels from validation data\n",
    "        self.val_texts = val_df[feature_col].tolist()\n",
    "        self.val_labels = val_df[label_col].values\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        self.tokenizer = Tokenizer() \n",
    "        self.tokenizer.fit_on_texts(self.train_texts) \n",
    "\n",
    "        # Converting text data to sequences\n",
    "        train_sequences = self.tokenizer.texts_to_sequences(self.train_texts)\n",
    "        val_sequences = self.tokenizer.texts_to_sequences(self.val_texts)\n",
    "\n",
    "        # Padding sequences to a fixed length\n",
    "        self.train_data = pad_sequences(train_sequences, maxlen=self.max_len, padding='post')\n",
    "        self.val_data = pad_sequences(val_sequences, maxlen=self.max_len, padding='post')\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model = Sequential() \n",
    "        self.model.add(Embedding(len(self.tokenizer.word_index) + 1, 128, input_length=self.max_len)) \n",
    "        self.model.add(Conv1D(128, 5, activation='relu')) \n",
    "        self.model.add(GlobalMaxPooling1D()) \n",
    "        self.model.add(Dense(64, activation='relu')) \n",
    "        self.model.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "        optimizer = Adam(learning_rate=self.learning_rate) \n",
    "        self.model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "    def evaluate_model(self):\n",
    "      loss, accuracy = self.model.evaluate(self.val_data, self.val_labels, verbose=0)\n",
    "      return accuracy\n",
    "\n",
    "    def train_model(self):\n",
    "        self.history = self.model.fit(self.train_data, self.train_labels, epochs=self.epochs, batch_size=self.batch_size, validation_data=(self.val_data, self.val_labels)) # Training the model\n",
    "\n",
    "    def get_training_loss(self):\n",
    "        return self.history.history['loss']\n",
    "\n",
    "    def get_validation_loss(self):\n",
    "        return self.history.history['val_loss']\n",
    "\n",
    "    def get_validation_accuracy(self):\n",
    "        return self.history.history['val_accuracy']\n",
    "\n",
    "    def save_model(self, save_dir, model_name):\n",
    "      os.makedirs(save_dir, exist_ok=True) \n",
    "      self.model.save(os.path.join(save_dir, model_name + '.keras')) \n",
    "\n",
    "# Usage\n",
    "start_time = time.time()\n",
    "model = 'cnn'\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 2e-5\n",
    "epochs = 3\n",
    "batch_size = 6\n",
    "max_len = 4096\n",
    "optimizer = 'Adam'\n",
    "\n",
    "# Paths and filenames\n",
    "absolute_path = \"./\"\n",
    "train_file_path = 'Datasets/train_set.csv' \n",
    "val_file_path = 'Datasets/validation_set.csv'\n",
    "save_dir = 'TrainedModels/'\n",
    "trained_model = model + '_optimizer_' + optimizer + '_lr_' + str(learning_rate) + '_epochs_' + str(epochs) + '_bs_' + str(batch_size) + '_maxlen_' + str(max_len)\n",
    "feature_col = 'text' \n",
    "label_col = 'label'\n",
    "\n",
    "# Training and saving a CNN model for spam classification\n",
    "trainer = CNNTraining(learning_rate, epochs, batch_size, max_len, feature_col, label_col) \n",
    "trainer.load_data(absolute_path + train_file_path, absolute_path + val_file_path) \n",
    "trainer.preprocess_data() \n",
    "trainer.build_model()\n",
    "trainer.train_model() \n",
    "trainer.save_model(absolute_path + save_dir, trained_model) \n",
    "\n",
    "# Time\n",
    "training_time = time.time() - start_time\n",
    "inference_start_time = time.time()\n",
    "validation_accuracy = trainer.evaluate_model()\n",
    "inference_time = time.time() - inference_start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vn5tJPMhstpd"
   },
   "source": [
    "# Predict using the fine-tuned RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5309,
     "status": "ok",
     "timestamp": 1714293410828,
     "user": {
      "displayName": "Roumeliotis Konstadinos",
      "userId": "17264923090131634662"
     },
     "user_tz": -180
    },
    "id": "7ApTFf0Ni3R6",
    "outputId": "c6d6eb4a-af40-4f8f-8640-8d7fcfc7c54c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
      "35/35 [==============================] - 0s 6ms/step\n",
      "Predictions done\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "class CNNPredictions:\n",
    "    def __init__(self, max_len, absolute_path, test_file_path, predictions_path, trained_model, feature_col, prediction_col):\n",
    "        self.max_len = max_len\n",
    "        self.test_file_path = test_file_path\n",
    "        self.predictions_path = predictions_path\n",
    "        self.absolute_path = absolute_path\n",
    "        self.trained_model = trained_model\n",
    "        self.feature_col = feature_col\n",
    "        self.prediction_col = prediction_col\n",
    "\n",
    "    def predict(self):\n",
    "      # Load test dataset\n",
    "      test_df = pd.read_csv(self.absolute_path + self.test_file_path)\n",
    "      test_texts = test_df[self.feature_col].tolist() \n",
    "\n",
    "      # Tokenize text data using the same tokenizer used during training\n",
    "      tokenizer = Tokenizer()  \n",
    "      tokenizer.fit_on_texts(test_texts)  \n",
    "      test_sequences = tokenizer.texts_to_sequences(test_texts) \n",
    "\n",
    "      test_data = pad_sequences(test_sequences, maxlen=self.max_len, padding='post')  \n",
    "\n",
    "      saved_model = load_model(self.absolute_path + self.trained_model)  \n",
    "\n",
    "      # Make predictions on test data using the loaded model\n",
    "      predictions = saved_model.predict(test_data)  \n",
    "      binary_predictions = (predictions > 0.5).astype(int)\n",
    "      test_df[self.prediction_col] = binary_predictions\n",
    "      test_df.to_csv(self.absolute_path + self.predictions_path, index=False)\n",
    "\n",
    "      print(\"Predictions done\")\n",
    "\n",
    "max_len = 4096\n",
    "str_params = 'cnn_optimizer_Adam_lr_2e-05_epochs_3_bs_6_maxlen_4096'\n",
    "\n",
    "# Paths and filenames\n",
    "absolute_path = \"./\"\n",
    "test_file_path = 'Datasets/test_set.csv' \n",
    "predictions_path = 'Datasets/test_set.csv' \n",
    "trained_model = 'TrainedModels/' + str_params + '.keras'\n",
    "feature_col = 'text'\n",
    "prediction_col = str_params + '_prediction'\n",
    "\n",
    "# Instantiate the CNNPredictions class\n",
    "cnn_predictions = CNNPredictions(max_len, absolute_path, test_file_path, predictions_path, trained_model, feature_col, prediction_col)\n",
    "\n",
    "# Perform predictions\n",
    "cnn_predictions.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Metrics \n",
    "def false_positive_rate(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return fp / (fp + tn)\n",
    "\n",
    "SCORING = {\n",
    "    \"F1\": f1_score,\n",
    "    \"Accuracy\": accuracy_score,\n",
    "    \"Precision\": precision_score,\n",
    "    \"Recall\": recall_score,\n",
    "    \"MCC\": matthews_corrcoef,\n",
    "    \"ROC AUC\": roc_auc_score,\n",
    "    \"PRC AREA\": average_precision_score,\n",
    "    \"FPR\": false_positive_rate, # Custom FPR function\n",
    "}\n",
    "\n",
    "# Load predictions\n",
    "predictions_df = pd.read_csv(\"Datasets/test_set.csv\")\n",
    "metric_values = {}\n",
    "\n",
    "# Evaluate metrics\n",
    "for metric_name, metric_func in SCORING.items():\n",
    "    if metric_name in [\"PRC AREA\", \"FPR\", \"ROC AUC\"]:\n",
    "        metric_values[metric_name] = metric_func(predictions_df[\"label\"], predictions_df[prediction_col])\n",
    "    else:\n",
    "        metric_values[metric_name] = metric_func(predictions_df[\"label\"], predictions_df[prediction_col])\n",
    "\n",
    "# Time\n",
    "metric_values[\"training_time\"] = training_time\n",
    "metric_values[\"inference_time\"] = inference_time\n",
    "\n",
    "columns = list(SCORING.keys()) + [\"training_time\", \"inference_time\"]\n",
    "scores = pd.DataFrame(columns=columns)\n",
    "\n",
    "row = {}\n",
    "for metric in SCORING.keys():\n",
    "    val = metric_values[metric]\n",
    "    row[metric] = round(val, 4) if isinstance(val, (float, int)) else val \n",
    "\n",
    "row[\"training_time\"] = round(metric_values.get(\"training_time\", 0), 4)\n",
    "row[\"inference_time\"] = round(metric_values.get(\"inference_time\", 0), 4)\n",
    "\n",
    "scores.loc[\"CNN\"] = row\n",
    "\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO7dCQL4UDyyEPnf9KsKll/",
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
